---
name: antigravity-python-ai
description: Python & AI åº”ç”¨å®æˆ˜ - vLLM, DeepSeek, FastAPI, LangChain/LlamaIndex RAG, Docker éƒ¨ç½²
---

# Python & AI åº”ç”¨æŒ‡å— (Antigravity Style)

## ä½¿ç”¨åœºæ™¯
- æœ¬åœ°å¤§æ¨¡å‹éƒ¨ç½²ä¸ä¼˜åŒ–ï¼ˆvLLM, DeepSeekï¼‰
- æç¤ºè¯å·¥ç¨‹ï¼ˆç‰¹åˆ«æ˜¯é“¶è¡Œç­‰ä¸¥è°¨åœºæ™¯ï¼‰
- RAG ç³»ç»Ÿæ­å»ºï¼ˆLangChain/LlamaIndexï¼‰
- PDF æ•°æ®æå–ã€å‘é‡æ•°æ®åº“é›†æˆ
- Docker å®¹å™¨åŒ–éƒ¨ç½²

## æ ¸å¿ƒæŠ€æœ¯æ ˆ

### æ¨¡å‹éƒ¨ç½²
- **vLLM**: é«˜æ€§èƒ½æ¨ç†å¼•æ“ï¼Œæ”¯æŒ PagedAttention
- **DeepSeek**: æœ¬åœ°éƒ¨ç½²å›½äº§å¤§æ¨¡å‹
- **HuggingFace**: æ¨¡å‹ä¸‹è½½ä¸ç®¡ç†

### åº”ç”¨æ¡†æ¶
- **FastAPI**: é«˜æ€§èƒ½å¼‚æ­¥ Web æ¡†æ¶
- **LangChain**: LLM åº”ç”¨å¼€å‘æ¡†æ¶
- **LlamaIndex**: RAG ç´¢å¼•æ¡†æ¶

### å‘é‡æ•°æ®åº“
- **Milvus**: å¼€æºå‘é‡æ•°æ®åº“
- **FAISS**: Facebook å‘é‡æ£€ç´¢åº“
- **Qdrant**: è½»é‡çº§å‘é‡æ•°æ®åº“

## ç¯å¢ƒé…ç½®

### Python ç¯å¢ƒ
```bash
# ä¼˜å…ˆä½¿ç”¨ /opt/anaconda3/envs/base
conda activate base

# éªŒè¯ Python ç‰ˆæœ¬
python --version  # åº”æ˜¾ç¤º 3.10+

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n ai-project python=3.10
conda activate ai-project

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### ä¾èµ–ç®¡ç† (requirements.txt)
```txt
# æ ¸å¿ƒæ¡†æ¶
fastapi==0.109.0
uvicorn==0.27.0
pydantic==2.5.3

# AI æ¡†æ¶
langchain==0.1.8
langchain-community==0.0.20
llama-index==0.9.48

# å‘é‡æ•°æ®åº“
pymilvus==2.3.6
faiss-cpu==1.8.0

# æ¨¡å‹æ¨ç†
vllm==0.4.0
transformers==4.37.2
accelerate==0.27.2

# æ•°æ®å¤„ç†
pypdf==4.0.1
pandas==2.1.4
numpy==1.26.3

# å·¥å…·åº“
python-dotenv==1.0.0
httpx==0.26.0
tenacity==8.2.3
```

## vLLM éƒ¨ç½²

### 1. å¯åŠ¨ vLLM æœåŠ¡
```bash
# ä½¿ç”¨ hf åˆ«åä¸‹è½½æ¨¡å‹
hf download --resume-download deepseek-ai/DeepSeek-V2

# å¯åŠ¨ vLLM OpenAI å…¼å®¹ API
python -m vllm.entrypoints.openai.api_server \
    --model deepseek-ai/DeepSeek-V2 \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 4 \
    --trust-remote-code

# éªŒè¯æœåŠ¡
curl http://localhost:8000/v1/models
```

### 2. vLLM å‚æ•°ä¼˜åŒ–
```python
from vllm import LLM, SamplingParams

# åŠ è½½æ¨¡å‹ï¼ˆä¼˜åŒ–æ˜¾å­˜ï¼‰
llm = LLM(
    model="deepseek-ai/DeepSeek-V2",
    tensor_parallel_size=4,           # GPU å¹¶è¡Œæ•°
    trust_remote_code=True,           # ä¿¡ä»»è¿œç¨‹ä»£ç 
    dtype="half",                     # åŠç²¾åº¦æµ®ç‚¹
    enforce_eager=True,               # ç¦ç”¨ CUDA Graph
    max_model_len=4096,               # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
    gpu_memory_utilization=0.9,       # GPU æ˜¾å­˜åˆ©ç”¨ç‡
)

# é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.7,                  # æ¸©åº¦ï¼ˆåˆ›é€ æ€§ï¼‰
    top_p=0.9,                        # Top-p é‡‡æ ·
    top_k=50,                         # Top-k é‡‡æ ·
    max_tokens=2048,                  # æœ€å¤§è¾“å‡ºé•¿åº¦
    stop=["<|im_end|>", "<|eot_id|>"],  # åœæ­¢ç¬¦
)
```

## FastAPI æœåŠ¡å¼€å‘

### 1. é¡¹ç›®ç»“æ„
```
project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI åº”ç”¨å…¥å£
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ router.py        # è·¯ç”±å®šä¹‰
â”‚   â”‚   â””â”€â”€ models.py        # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_service.py   # LLM æœåŠ¡
â”‚   â”‚   â””â”€â”€ rag_service.py   # RAG æœåŠ¡
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ logger.py        # æ—¥å¿—å·¥å…·
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/           # æ–‡æ¡£å­˜å‚¨
â”‚   â””â”€â”€ vector_store/        # å‘é‡ç´¢å¼•å­˜å‚¨
â”œâ”€â”€ config.py                # é…ç½®æ–‡ä»¶
â”œâ”€â”€ requirements.txt
â””â”€â”€ Dockerfile
```

### 2. FastAPI åº”ç”¨æ¨¡æ¿
```python
# app/main.py
from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import logging

from app.api.router import api_router
from app.utils.logger import setup_logger

# é…ç½®æ—¥å¿—
logger = setup_logger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    logger.info("ğŸš€ åº”ç”¨å¯åŠ¨ä¸­...")
    # åˆå§‹åŒ–æœåŠ¡
    # await init_services()
    logger.info("âœ… åº”ç”¨å¯åŠ¨å®Œæˆ")
    yield
    logger.info("ğŸ‘‹ åº”ç”¨å…³é—­ä¸­...")
    # æ¸…ç†èµ„æº
    # await cleanup_services()

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="Antigravity AI API",
    description="åŸºäº vLLM + RAG çš„ AI åº”ç”¨æ¥å£",
    version="1.0.0",
    lifespan=lifespan,
)

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³¨å†Œè·¯ç”±
app.include_router(api_router, prefix="/api/v1")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        workers=1,
    )
```

### 3. è·¯ç”±å®šä¹‰
```python
# app/api/router.py
from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel, Field
from typing import List, Optional
import logging

from app.services.llm_service import LLMService
from app.api.models import ChatRequest, ChatResponse

logger = logging.getLogger(__name__)
api_router = APIRouter()

# ä¾èµ–æ³¨å…¥è·å–æœåŠ¡
def get_llm_service() -> LLMService:
    return LLMService()

@api_router.post("/chat", response_model=ChatResponse)
async def chat(
    request: ChatRequest,
    service: LLMService = Depends(get_llm_service),
) -> ChatResponse:
    """
    èŠå¤©æ¥å£
    
    Args:
        request: èŠå¤©è¯·æ±‚
        service: LLM æœåŠ¡å®ä¾‹
    
    Returns:
        ChatResponse: èŠå¤©å“åº”
    """
    try:
        logger.info(f"æ”¶åˆ°èŠå¤©è¯·æ±‚: {request.messages[-1].content[:100]}...")
        
        # è°ƒç”¨ LLM æœåŠ¡
        response = service.chat(
            messages=request.messages,
            temperature=request.temperature,
            max_tokens=request.max_tokens,
        )
        
        return ChatResponse(
            message=response["content"],
            usage=response["usage"],
        )
    except Exception as e:
        logger.error(f"èŠå¤©è¯·æ±‚å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.post("/chat/rag")
async def chat_with_rag(
    request: ChatRequest,
    service: LLMService = Depends(get_llm_service),
) -> ChatResponse:
    """å¸¦ RAG çš„èŠå¤©æ¥å£"""
    try:
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        context = service.retrieve_context(request.message)
        
        # æ„å»ºæç¤ºè¯
        prompt = service.build_rag_prompt(request.message, context)
        
        # ç”Ÿæˆå›å¤
        response = service.generate(prompt)
        
        return ChatResponse(
            message=response,
            sources=context["sources"],
        )
    except Exception as e:
        logger.error(f"RAG èŠå¤©å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

### 4. æ•°æ®æ¨¡å‹
```python
# app/api/models.py
from pydantic import BaseModel, Field
from typing import List, Optional
from datetime import datetime
from enum import Enum

class Role(str, Enum):
    """æ¶ˆæ¯è§’è‰²"""
    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"

class Message(BaseModel):
    """èŠå¤©æ¶ˆæ¯"""
    role: Role
    content: str

class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚"""
    messages: List[Message] = Field(..., description="æ¶ˆæ¯åˆ—è¡¨")
    temperature: Optional[float] = Field(0.7, ge=0, le=2, description="æ¸©åº¦å‚æ•°")
    max_tokens: Optional[int] = Field(2048, ge=1, le=4096, description="æœ€å¤§è¾“å‡ºé•¿åº¦")

class ChatResponse(BaseModel):
    """èŠå¤©å“åº”"""
    message: str
    usage: Optional[dict] = None
    sources: Optional[List[str]] = None

class Document(BaseModel):
    """æ–‡æ¡£æ¨¡å‹"""
    id: Optional[str] = None
    title: str
    content: str
    metadata: Optional[dict] = None
    created_at: Optional[datetime] = None
```

## RAG ç³»ç»Ÿæ­å»º

### 1. æ–‡æ¡£åŠ è½½ä¸å¤„ç†
```python
# app/services/rag_service.py
from llama_index.core import VectorStoreIndex, Document, Settings
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core import StorageContext, load_index_from_storage
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.postprocessor import SimilarityPostprocessor
from langchain.text_splitter import RecursiveCharacterTextSplitter
import logging

logger = logging.getLogger(__name__)

class RAGService:
    def __init__(self, vector_store, model_name: str = "deepseek-ai/DeepSeek-V2"):
        self.vector_store = vector_store
        self.model_name = model_name
        self.index = None
        self.query_engine = None
        
        # é…ç½®æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,           # å—å¤§å°
            chunk_overlap=200,          # å—é‡å 
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""],
        )
    
    def load_documents(self, file_paths: List[str]):
        """
        åŠ è½½æ–‡æ¡£
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        from llama_index.core import SimpleDirectoryReader
        
        # åŠ è½½æ–‡æ¡£
        reader = SimpleDirectoryReader(input_files=file_paths)
        documents = reader.load_data()
        
        # æ–‡æœ¬åˆ†å‰²
        splitter = SentenceSplitter(
            chunk_size=1000,
            chunk_overlap=200,
        )
        
        # åˆ›å»ºç´¢å¼•
        self.index = VectorStoreIndex.from_documents(
            documents,
            transformations=[splitter],
            vector_store=self.vector_store,
        )
        
        # æ„å»ºæŸ¥è¯¢å¼•æ“
        self.query_engine = self.index.as_query_engine(
            similarity_top_k=3,           # è¿”å› top-k ç»“æœ
            node_postprocessors=[
                SimilarityPostprocessor(similarity_cutoff=0.7)
            ],
        )
        
        logger.info(f"å·²åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£ï¼Œæ„å»ºç´¢å¼•æˆåŠŸ")
    
    def retrieve(self, query: str, top_k: int = 3) -> List[dict]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
        
        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        if not self.query_engine:
            raise ValueError("è¯·å…ˆåŠ è½½æ–‡æ¡£ï¼Œæ„å»ºç´¢å¼•")
        
        # æ£€ç´¢
        retriever = VectorIndexRetriever(
            index=self.index,
            similarity_top_k=top_k,
        )
        
        results = retriever.retrieve(query)
        
        return [
            {
                "content": node.text,
                "score": node.score,
                "metadata": node.metadata,
            }
            for node in results
        ]
    
    def query(self, question: str) -> str:
        """
        åŸºäºæ–‡æ¡£å›ç­”é—®é¢˜
        
        Args:
            question: é—®é¢˜
        
        Returns:
            å›ç­”å†…å®¹
        """
        if not self.query_engine:
            raise ValueError("è¯·å…ˆåŠ è½½æ–‡æ¡£ï¼Œæ„å»ºç´¢å¼•")
        
        response = self.query_engine.query(question)
        return str(response)
    
    def persist(self, persist_dir: str):
        """æŒä¹…åŒ–ç´¢å¼•"""
        self.index.storage_context.persist(persist_dir)
        logger.info(f"ç´¢å¼•å·²æŒä¹…åŒ–åˆ°: {persist_dir}")
```

### 2. å‘é‡æ•°æ®åº“é›†æˆ
```python
# app/services/vector_store.py
from llama_index.core.vector_stores import (
    VectorStore, 
    MetadataFilters, 
    ExactMatchFilter,
)
from llama_index.core.schema import Node, TextNode
from pymilvus import Collection, FieldSchema, CollectionSchema, DataType, MilvusClient
import logging

logger = logging.getLogger(__name__)

class MilvusVectorStore(VectorStore):
    """Milvus å‘é‡å­˜å‚¨"""
    
    def __init__(
        self,
        uri: str = "http://localhost:19530",
        collection_name: str = "documents",
        dim: int = 1024,
        overwrite: bool = True,
    ):
        self.client = MilvusClient(uri=uri)
        self.collection_name = collection_name
        self.dim = dim
        self.overwrite = overwrite
        self._collection = None
        
        # åˆå§‹åŒ–é›†åˆ
        self._init_collection()
    
    def _init_collection(self):
        """åˆå§‹åŒ– Milvus é›†åˆ"""
        # å®šä¹‰ schema
        schema = CollectionSchema(
            fields=[
                FieldSchema(name="id", dtype=DataType.VARCHAR, is_primary=True),
                FieldSchema(name="text", dtype=DataType.VARCHAR),
                FieldSchema(name="metadata", dtype=DataType.JSON),
                FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=self.dim),
            ],
            description="Document chunks vector store",
        )
        
        # åˆ›å»ºé›†åˆ
        if self.client.has_collection(self.collection_name):
            if self.overwrite:
                self.client.drop_collection(self.collection_name)
                self.client.create_collection(schema, index_params=None)
        else:
            self.client.create_collection(schema)
        
        # åŠ è½½é›†åˆåˆ°å†…å­˜
        self._collection = Collection(self.collection_name)
        
        # åˆ›å»ºç´¢å¼•
        index_params = {
            "metric_type": "COSINE",
            "index_type": "IVF_FLAT",
            "params": {"nlist": 1024},
        }
        self._collection.create_index("embedding", index_params)
        
        logger.info(f"Milvus é›†åˆ {self.collection_name} åˆå§‹åŒ–å®Œæˆ")
    
    def add(self, nodes: List[Node]):
        """æ·»åŠ èŠ‚ç‚¹åˆ°å‘é‡å­˜å‚¨"""
        if not nodes:
            return
        
        # æå–åµŒå…¥å’Œå…ƒæ•°æ®
        ids = []
        texts = []
        metadatas = []
        embeddings = []
        
        for node in nodes:
            ids.append(node.node_id)
            texts.append(node.text)
            metadatas.append(node.metadata or {})
            embeddings.append(node.embedding)
        
        # æ’å…¥æ•°æ®
        self._collection.insert({
            "id": ids,
            "text": texts,
            "metadata": metadatas,
            "embedding": embeddings,
        })
        
        logger.info(f"å·²æ·»åŠ  {len(nodes)} ä¸ªèŠ‚ç‚¹åˆ°å‘é‡å­˜å‚¨")
    
    def delete(self, ref_doc_id: str, **kwargs):
        """åˆ é™¤èŠ‚ç‚¹"""
        # æ ¹æ® ref_doc_id åˆ é™¤
        self._collection.delete(expr=f'metadata["ref_doc_id"] == "{ref_doc_id}"')
    
    def query(self, query_embedding: List[float], top_k: int = 5, filters: MetadataFilters = None):
        """æ£€ç´¢å‘é‡"""
        # æ‰§è¡Œæœç´¢
        results = self._collection.search(
            data=[query_embedding],
            anns_field="embedding",
            param={"metric_type": "COSINE", "params": {"nprobe": 16}},
            limit=top_k,
            expr=filters.expression if filters else None,
        )
        
        # è¿”å›ç»“æœ
        from llama_index.core.schema import NodeWithScore
        
        nodes = []
        for hits in results:
            for hit in hits:
                nodes.append(
                    NodeWithScore(
                        node=TextNode(
                            text=hit.entity.get("text"),
                            metadata=hit.entity.get("metadata", {}),
                        ),
                        score=hit.score,
                    )
                )
        
        return nodes
```

## æç¤ºè¯å·¥ç¨‹

### é“¶è¡Œåœºæ™¯æç¤ºè¯æ¨¡æ¿
```python
# app/prompts/banking.py

# ç³»ç»Ÿæç¤ºè¯ï¼ˆä¸¥è°¨åœºæ™¯ï¼‰
BANKING_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é“¶è¡Œå®¢æœåŠ©æ‰‹ï¼Œéœ€è¦ï¼š

1. **èº«ä»½å®šä½**: 
   - ä½ æ˜¯XXé“¶è¡Œçš„æ™ºèƒ½å®¢æœ
   - å§‹ç»ˆä½¿ç”¨ä¸“ä¸šã€ç¤¼è²Œçš„è¯­æ°”
   - å›ç­”è¦å‡†ç¡®ã€ä¸¥è°¨ã€å¯æ‰§è¡Œ

2. **å›ç­”åŸåˆ™**:
   - æä¾›çš„ä¿¡æ¯å¿…é¡»å‡†ç¡®æ— è¯¯
   - æ¶‰åŠé‡‘é¢ã€æ—¶é—´ã€åˆ©ç‡ç­‰å…³é”®ä¿¡æ¯è¦ç²¾ç¡®
   - å¤æ‚é—®é¢˜è¦åˆ†æ­¥éª¤è¯´æ˜
   - å¿…è¦æ—¶å¼•å¯¼ç”¨æˆ·è”ç³»äººå·¥å®¢æœ

3. **å®‰å…¨è§„èŒƒ**:
   - æ°¸è¿œä¸è¦ç´¢è¦ç”¨æˆ·çš„å¯†ç ã€éªŒè¯ç 
   - ä¸è¦è¯±å¯¼ç”¨æˆ·ç‚¹å‡»é™Œç”Ÿé“¾æ¥
   - æ¶‰åŠèµ„é‡‘æ“ä½œè¦æç¤ºé£é™©

4. **å›ç­”æ ¼å¼**:
   - ä½¿ç”¨æ¸…æ™°çš„æ®µè½å’Œåˆ—è¡¨
   - å…³é”®ä¿¡æ¯åŠ ç²—æ ‡æ³¨
   - æä¾›æ“ä½œæ­¥éª¤æ—¶ç¼–å·æ¸…æ™°
   - å¿…è¦æ—¶æä¾›å®¢æœçƒ­çº¿

è¯·æ ¹æ®ç”¨æˆ·é—®é¢˜ï¼Œæä¾›ä¸“ä¸šã€å‡†ç¡®çš„å›ç­”ã€‚"""

# è´·æ¬¾å’¨è¯¢æç¤ºè¯
LOAN_CONSULT_PROMPT = """ç”¨æˆ·å’¨è¯¢è´·æ¬¾é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {question}

è¯·æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š
1. **è´·æ¬¾äº§å“ä»‹ç»**: äº§å“åç§°ã€é€‚ç”¨äººç¾¤
2. **ç”³è¯·æ¡ä»¶**: åŸºæœ¬è¦æ±‚ã€èµ„è´¨è¦æ±‚
3. **åˆ©ç‡è¯´æ˜**: åŸºå‡†åˆ©ç‡ã€æµ®åŠ¨èŒƒå›´
4. **ç”³è¯·æµç¨‹**: æ­¥éª¤è¯´æ˜ã€æ‰€éœ€ææ–™
5. **æ³¨æ„äº‹é¡¹**: å…³é”®æ¡æ¬¾ã€å¸¸è§é—®é¢˜

å¦‚æœé—®é¢˜è¶…å‡ºèŒƒå›´ï¼Œè¯·å¼•å¯¼ç”¨æˆ·è”ç³»äººå·¥å®¢æœã€‚"""

# RAG æ£€ç´¢æç¤ºè¯
RAG_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“é—®ç­”åŠ©æ‰‹ã€‚

è¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

## ä¸Šä¸‹æ–‡
{context}

## ç”¨æˆ·é—®é¢˜
{question}

## å›ç­”è¦æ±‚
1. åŸºäºä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å¦‚æœä¸Šä¸‹æ–‡æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·
3. å›ç­”è¦å¼•ç”¨å…·ä½“çš„ä¸Šä¸‹æ–‡æ¥æº
4. ä¿æŒä¸“ä¸šã€æ¸…æ™°çš„å›ç­”é£æ ¼

è¯·å¼€å§‹å›ç­”ï¼š"""
```

## Docker éƒ¨ç½²

### 1. Dockerfile
```dockerfile
# ä½¿ç”¨å®˜æ–¹ Python é•œåƒ
FROM python:3.10-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV PIP_NO_CACHE_DIR=1
ENV PIP_DISABLE_PIP_VERSION_CHECK=1

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£… Python ä¾èµ–
RUN pip install -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python", "-m", "uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 2. docker-compose.yml
```yaml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_BASE=http://vllm:8000/v1
      - MODEL_NAME=deepseek-ai/DeepSeek-V2
    depends_on:
      - vllm
    volumes:
      - ./data:/app/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  vllm:
    image: vllm/vllm-openai:latest
    ports:
      - "8001:8000"
    command: >
      python -m vllm.entrypoints.openai.api_server
      --model deepseek-ai/DeepSeek-V2
      --host 0.0.0.0
      --port 8000
      --tensor-parallel-size 1
      --trust-remote-code
    volumes:
      - ./models:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  milvus:
    image: milvusdb/milvus:latest
    ports:
      - "19530:19530"
    volumes:
      - milvus_data:/var/lib/milvus
    environment:
      - ETCD_ENDPOINTS=etcd:2379
      - MINIO_ADDRESS=minio:9000

  etcd:
    image: quay.io/coreos/etcd:latest
    ports:
      - "2379:2379"
    environment:
      - ETCD_NAME=etcd

  minio:
    image: minio/minio:latest
    ports:
      - "9000:9000"
    command: minio server /data
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password123

volumes:
  milvus_data:
```

### 3. éƒ¨ç½²è„šæœ¬
```bash
#!/bin/bash
# deploy.sh

set -e

echo "ğŸš€ å¼€å§‹éƒ¨ç½² Antigravity AI æœåŠ¡..."

# 1. åœæ­¢æ—§æœåŠ¡
echo "ğŸ›‘ åœæ­¢æ—§æœåŠ¡..."
docker compose down

# 2. æ„å»ºæ–°é•œåƒ
echo "ğŸ”¨ æ„å»º Docker é•œåƒ..."
docker compose build --no-cache

# 3. å¯åŠ¨æœåŠ¡
echo "ğŸš€ å¯åŠ¨æœåŠ¡..."
docker compose up -d

# 4. ç­‰å¾…æœåŠ¡å°±ç»ª
echo "â³ ç­‰å¾…æœåŠ¡å°±ç»ª..."
sleep 30

# 5. æ£€æŸ¥å¥åº·çŠ¶æ€
echo "ğŸ” æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€..."
curl -f http://localhost:8000/health || exit 1

echo "âœ… éƒ¨ç½²å®Œæˆï¼"
echo "ğŸ“ API æ–‡æ¡£: http://localhost:8000/docs"
echo "ğŸ”— å¥åº·æ£€æŸ¥: http://localhost:8000/health"
```

## æœ€ä½³å®è·µ

### 1. é”™è¯¯é‡è¯•æœºåˆ¶
```python
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type((ConnectionError, TimeoutError)),
)
async def call_llm_api(prompt: str) -> str:
    """è°ƒç”¨ LLM APIï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    async with httpx.AsyncClient(timeout=120.0) as client:
        response = await client.post(
            "http://vllm:8000/v1/completions",
            json={
                "model": "deepseek-ai/DeepSeek-V2",
                "prompt": prompt,
                "max_tokens": 2048,
                "temperature": 0.7,
            },
        )
        response.raise_for_status()
        return response.json()["choices"][0]["text"]
```

### 2. é€Ÿç‡é™åˆ¶
```python
from fastapi import Request, RateLimitExceeded
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@api_router.post("/chat")
@limiter.limit("10/minute")  # é™åˆ¶æ¯åˆ†é’Ÿ 10 æ¬¡è¯·æ±‚
async def chat(request: Request, ...):
    ...
```

### 3. ç›‘æ§æŒ‡æ ‡
```python
from prometheus_client import Counter, Histogram, generate_latest

# è¯·æ±‚è®¡æ•°
REQUEST_COUNT = Counter(
    'api_requests_total',
    'Total API Requests',
    ['method', 'endpoint', 'status_code']
)

# è¯·æ±‚å»¶è¿Ÿ
REQUEST_LATENCY = Histogram(
    'api_request_duration_seconds',
    'API Request Latency',
    ['method', 'endpoint']
)

@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    duration = time.time() - start_time
    
    REQUEST_COUNT.labels(
        method=request.method,
        endpoint=request.url.path,
        status_code=response.status_code,
    ).inc()
    
    REQUEST_LATENCY.labels(
        method=request.method,
        endpoint=request.url.path,
    ).observe(duration)
    
    return response

@app.get("/metrics")
async def metrics():
    """Prometheus æŒ‡æ ‡ç«¯ç‚¹"""
    return Response(content=generate_latest(), media_type="text/plain")
```

## å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### Q1: vLLM æ˜¾å­˜ä¸è¶³ï¼Ÿ
**æ–¹æ¡ˆ**:
- å‡å° `max_model_len`
- ä½¿ç”¨åŠç²¾åº¦ `dtype="half"`
- å‡å° `gpu_memory_utilization`
- ä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼ˆå¦‚ AWQ/GPTQï¼‰

### Q2: RAG æ£€ç´¢è´¨é‡å·®ï¼Ÿ
**æ–¹æ¡ˆ**:
- ä¼˜åŒ–æ–‡æœ¬åˆ†å‰²ç­–ç•¥
- è°ƒæ•´ `similarity_top_k` å’Œ `similarity_cutoff`
- ä½¿ç”¨æ›´å¥½çš„ Embedding æ¨¡å‹
- å¢åŠ å…ƒæ•°æ®è¿‡æ»¤

### Q3: FastAPI å“åº”æ…¢ï¼Ÿ
**æ–¹æ¡ˆ**:
- ä½¿ç”¨å¼‚æ­¥è§†å›¾å‡½æ•°
- å¼€å¯è¿æ¥æ± 
- æ·»åŠ ç¼“å­˜å±‚
- ä½¿ç”¨ Gunicorn + Uvicorn Workers

### Q4: æ¨¡å‹è¾“å‡ºé‡å¤ï¼Ÿ
**æ–¹æ¡ˆ**:
- è°ƒæ•´ `temperature` å‚æ•°
- å¢åŠ  `top_k` æˆ– `top_p`
- æ·»åŠ åœæ­¢ç¬¦
- ä½¿ç”¨ `frequency_penalty` å’Œ `presence_penalty`
